{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "proprietary-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, re, string, gc\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from shopeenet import ShopeeNet\n",
    "from shopeedataset import ShopeeDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 12\n",
    "SEED = 24\n",
    "\n",
    "MODEL_PATH = 'checkpoints/arcface_epoch9.pth'\n",
    "MODEL_PARAMS = {\n",
    "    'feature_space' : 680, \n",
    "    'out_features' : 11014, \n",
    "    'scale' : 24.0, \n",
    "    'margin' : 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lovely-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "train_path = 'data/train_images/'\n",
    "test_path = 'data/test_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detailed-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + 'train.csv')\n",
    "df['image'] = train_path + df['image']\n",
    "tmp = df.groupby('label_group').posting_id.unique().to_dict()\n",
    "df['target'] = df.label_group.map(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "informed-catalyst",
   "metadata": {},
   "source": [
    "# **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetric(col):\n",
    "    def f1score(row):\n",
    "        n = len(np.intersect1d(row.target, row[col]))\n",
    "        return 2 * n / (len(row.target) + len(row[col]))\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-interface",
   "metadata": {},
   "source": [
    "# **Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lined-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(df, mode, pretrained=False):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeNet(**MODEL_PARAMS)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location='cuda:0'))\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    image_transforms = A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    image_dataset = ShopeeDataset(df, mode, image_transforms)\n",
    "\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle = False,\n",
    "        pin_memory = True,\n",
    "        drop_last = False,\n",
    "        num_workers = N_WORKERS\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            feat = model(img)\n",
    "            feat = feat.reshape(feat.shape[0], feat.shape[1])\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    image_embeddings = normalize(image_embeddings)    \n",
    "    print(f'Image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_predictions(df, embeddings, threshold = 0.54):\n",
    "    \n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 3\n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    predictions = []\n",
    "    for dist, idx in tqdm(zip(distances, indices)):\n",
    "        posting_ids = df.iloc[cupy.asnumpy(idx[dist < 0.54])].posting_id.values\n",
    "        predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices, dist, idx, posting_ids\n",
    "    gc.collect()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2855.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45b3bab246c9456bb0ddefd9069efd3e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Image embeddings shape is (34250, 11014)\n"
     ]
    }
   ],
   "source": [
    "img_embs = get_image_embeddings(df, 'test', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions = get_image_predictions(df, img_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd047e50a5e036efca4439ea347f0ac18948c583c5fabaab04aa66ca000a7bf3349",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}