{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndata_dir = '../input/pretrained/'\ncache_dir ='/root/.cache/torch/hub/checkpoints/'\n\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\n\nfrom shutil import copyfile\nfor fname in os.listdir(data_dir):    \n    src = data_dir + fname\n    dest = cache_dir + fname\n    copyfile(src, dest)","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/effnetpytrochwhl/efficientnet_pytorch-0.7.0-py3-none-any.whl","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/effnetpytrochwhl/efficientnet_pytorch-0.7.0-py3-none-any.whl\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.7.0) (1.7.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.7.0) (1.19.5)\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import random, re, string, gc, math, os\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport sklearn\n# from sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\n\n\nfrom tqdm.notebook import tqdm\n\n\nimport  cuml, cupy, cudf\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nfrom cuml.experimental.preprocessing import normalize\n\n\nfrom PIL import Image\n\nimport torch\n# torch.manual_seed(1010)\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.dataloader import DataLoader\n\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom efficientnet_pytorch import EfficientNet\n\n\nimport nltk\nfrom nltk.corpus import stopwords","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 512\nN_WORKERS = 4\nBATCH_SIZE = 12\nSEED = 24\n\nMODEL_PATH = '../input/checkpoints/arcface_epoch9.pth'\nMODEL_PARAMS = {\n    'feature_space' : 680, \n    'out_features' : 11014, \n    'scale' : 24.0, \n    'margin' : 0.4\n}","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Utils**","metadata":{}},{"cell_type":"code","source":"def combine_for_sub(row):\n    x = np.concatenate([row.img_preds, row.text_preds])\n    return \" \".join(np.unique(x))","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(SEED)","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Image**\n","metadata":{}},{"cell_type":"code","source":"# class ShopeeDataset(Dataset):\n#     def __init__(self, img_path, transform):\n#         self.img_path = img_path\n#         self.transform = transform\n        \n#     def __getitem__(self, idx):\n#         img = Image.open(self.img_path[idx]).convert('RGB')\n#         img = self.transform(img)\n#         return img\n    \n#     def __len__(self):\n#         return len(self.img_path)\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, df, mode, transform):\n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        img = cv2.imread(row.image)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.transform(image=img)['image']\n\n        if self.mode == 'test':\n            return img\n        if self.mode == 'train':\n            return img, torch.tensor(row.label_group).float()\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ArcFace(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.5, eps=1e-6):\n        super(ArcFace, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.threshold = math.pi - margin\n        self.eps = eps\n\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, input, label=None):\n        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\n\n        if label is None:\n            return cos_theta\n\n        theta = torch.acos(torch.clamp(cos_theta, -1.0 + self.eps, 1.0 - self.eps))\n\n        one_hot = torch.zeros_like(cos_theta)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n\n        mask = torch.where(theta > self.threshold, torch.zeros_like(one_hot), one_hot)\n\n        logits = torch.cos(torch.where(mask.bool(), theta + self.margin, theta))\n        logits *= self.scale\n\n        return logits\n        \n\nclass ShopeeNet(nn.Module):\n    def __init__(self, feature_space, out_features, scale, margin):\n        super(ShopeeNet, self).__init__()\n        self.feature_space = feature_space\n        self.out_features = out_features\n        \n        self.backbone = EfficientNet.from_pretrained('efficientnet-b0')\n        in_features = self.backbone._conv_head.out_channels\n        self.dropout = nn.Dropout(p=self.backbone._global_params.dropout_rate)\n        self.classifier = nn.Linear(in_features, self.feature_space)\n        self.bn = nn.BatchNorm1d(self.feature_space)\n        \n        self.margin = ArcFace(\n            in_features = self.feature_space,\n            out_features = self.out_features,\n            scale = scale, \n            margin = margin       \n        )\n        \n        if self.training:\n            self._init_params()\n        \n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n\n    def forward(self, img, label=None):\n        batch_size = img.shape[0]\n        out = self.backbone.extract_features(img)\n        out = self.backbone._avg_pooling(out).view(batch_size, -1)\n        out = self.dropout(out)\n        out = self.classifier(out)\n        out = self.bn(out) \n        \n        if self.training:\n            logits = self.margin(out, label)\n            return logits\n        else:\n            logits = self.margin(out)\n            return logits\n        \n# class ShopeeNet(nn.Module):\n#     def __init__(self):\n#         super(ShopeeNet, self).__init__()\n#         model = EfficientNet.from_pretrained('efficientnet-b0')\n#         model.eval()\n#         self.model = model\n        \n#     def forward(self, img):\n#         out = self.model.extract_features(img)\n#         out = self.model._avg_pooling(out)\n#         return out","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_image_predictions(df, embeddings, threshold = 0.54):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for dist, idx in tqdm(zip(distances, indices)):\n        posting_ids = df.iloc[cupy.asnumpy(idx[dist < threshold])].posting_id.values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices, dist, idx, posting_ids\n    gc.collect()\n    return predictions","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(pretrained=False):\n    image_embeddings = []\n    \n    model = ShopeeNet(**MODEL_PARAMS)\n    if pretrained:\n        model.load_state_dict(torch.load(MODEL_PATH, map_location='cuda:0'))\n    model = model.to('cuda')   \n    \n    with torch.no_grad():\n        for img in tqdm(image_loader): \n            img = img.cuda()\n            feat = model(img)\n            feat = feat.reshape(feat.shape[0], feat.shape[1])\n            embed = cupy.asarray(feat)\n#             embed = feat.detach().cpu().numpy()\n            embed = normalize(embed)\n            image_embeddings.append(embed)\n            \n    \n    \n    del model, img, feat, embed\n    gc.collect()\n\n    image_embeddings = cupy.concatenate(image_embeddings)\n#     del embeds\n#     image_embeddings = normalize(image_embeddings)    \n    print(f'Image embeddings shape is {image_embeddings.shape}')\n    return image_embeddings","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Text**","metadata":{}},{"cell_type":"code","source":"def clean_title(row):\n    title = row.title\n    stop = stopwords.words('english')\n    title = [x for x in title.split() if not x in stop]\n    title = \" \".join(title)\n    title = title.lower()\n    title = re.sub(r\"\\-\",\"\",title)\n    title = re.sub(r\"\\+\",\"\",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\"\",title)\n    title = re.sub(r\"\\\\\",\"\",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\"\",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_text_predictions(df, max_features = 25_000, threshold = 0.54):\n    \n    text = cudf.Series.from_pandas(df.apply(clean_title, axis=1))    \n    model = TfidfVectorizer(stop_words=None, binary=True, max_features=max_features)\n    text_embeddings = model.fit_transform(text).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        distances = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            idx = cupy.where(distances[k] > threshold)[0]\n            o = df.iloc[cupy.asnumpy(idx)].posting_id.values\n            preds.append(o)\n    \n    del text, model, text_embeddings, distances, idx, o\n    gc.collect()\n    return preds","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions**","metadata":{}},{"cell_type":"code","source":"path = '../input/shopee-product-matching/'","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path + 'test.csv')\ndf['image'] = path + 'test_images/' + df['image']\n\n# df = pd.read_csv(path + 'train.csv')\n# df['image'] = path + 'train_images/' + df['image']\n# df = pd.concat([df,df,df.iloc[:10_000]])","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"image_transforms = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(),\n    ToTensorV2()\n])\n\nimage_dataset = ShopeeDataset(df, 'test', image_transforms)\n\nimage_loader = torch.utils.data.DataLoader(\n    image_dataset,\n    batch_size = BATCH_SIZE,\n    shuffle = False,\n    pin_memory = True,\n    drop_last = False,\n    num_workers = N_WORKERS\n)","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"image_embeddings = get_image_embeddings(pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6542 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba6daf4d07d49f78503a9859d8c69b3"}},"metadata":{}}]},{"cell_type":"code","source":"image_predictions = get_image_predictions(df, image_embeddings, threshold=0.94)\ndf['img_preds'] = image_predictions","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"del image_embeddings, image_predictions\ngc.collect()","metadata":{"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"22"},"metadata":{}}]},{"cell_type":"code","source":"text_predictions = get_text_predictions(df, threshold=0.7)\ndf['text_preds'] = text_predictions","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/cudf/core/join/join.py:368: UserWarning: can't safely cast column from right with type int32 to uint16, upcasting to int32\n  \"right\", dtype_r, dtype_l, libcudf_join_type\n","output_type":"stream"},{"name":"stdout","text":"Finding similar titles...\nchunk 0 to 4096\nchunk 4096 to 8192\nchunk 8192 to 12288\nchunk 12288 to 16384\nchunk 16384 to 20480\nchunk 20480 to 24576\nchunk 24576 to 28672\nchunk 28672 to 32768\nchunk 32768 to 36864\nchunk 36864 to 40960\nchunk 40960 to 45056\nchunk 45056 to 49152\nchunk 49152 to 53248\nchunk 53248 to 57344\nchunk 57344 to 61440\nchunk 61440 to 65536\nchunk 65536 to 69632\nchunk 69632 to 73728\nchunk 73728 to 77824\nchunk 77824 to 78500\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"df['matches'] = df.apply(combine_for_sub, axis = 1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}